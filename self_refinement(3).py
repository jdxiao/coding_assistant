# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tf4pQuBhwiOr9-9dvP_r_5U01zKN_ZQS
"""

def self_refine_until_success(problem_text, model, tokenizer, max_loops=3, max_new_tokens=300):
    prompt = f"### Problem:\n{problem_text}\n\n### Please solve this problem in Python."
    current_prompt = prompt
    for attempt in range(max_loops):
        inputs = tokenizer(current_prompt, return_tensors="pt").to(model.device)
        output = model.generate(
            inputs.input_ids,
            max_new_tokens=max_new_tokens,
            temperature=0.2,
            pad_token_id=tokenizer.eos_token_id
        )
        code = tokenizer.decode(output[0], skip_special_tokens=True)

        try:
            exec(code, {})
            print(f"Success on attempt {attempt + 1}")
            return code
        except Exception as e:
            error_message = str(e)
            print(f"Error on attempt {attempt + 1}: {error_message}")
            current_prompt = (
                f"{prompt}\n\n"
                f"### Previous code:\n{code}\n\n"
                f"### Error message:\n{error_message}\n\n"
                f"Please refine the code to fix the above error and improve readability if possible."
            )

    return code

def construct_prompt(problem_text, problem_type=None):
    base_prompt = f"""### Problem
{problem_text}

### Requirements
- Write clean, efficient Python code
- Include detailed inline comments
- Return a function only; no extra print statements

"""
    if problem_type == "array":
        base_prompt += "- Handle edge cases like empty arrays or duplicates\n"
    elif problem_type == "string":
        base_prompt += "- Optimize string operations and consider edge cases\n"
    elif problem_type == "graph":
        base_prompt += "- You may consider DFS or BFS traversal if needed\n"
    return base_prompt
# Example test
# print(construct_prompt("Given an array of nums...", problem_type="array"))

def analyze_generated_code(code):
    lines = code.strip().split("\n")
    num_comments = sum(1 for l in lines if l.strip().startswith("#"))
    has_docstring = '"""' in code or "'''" in code
    print(f"Total lines: {len(lines)}")
    print(f"Comments found: {num_comments}")
    print(f"Docstring present: {has_docstring}")
    return num_comments

# Install required packages
!pip install transformers datasets huggingface_hub

# Login to Hugging Face Hub (replace with your own token)
from huggingface_hub import login
login(token="hf_xxx...")  # replace with your own token  # Token removed for security  # â† replace with your real token

# Load necessary libraries
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch
import time

# Load the LeetCode dataset
ds = load_dataset("greengerong/leetcode")
train_dataset = ds['train']

# Print first sample (optional)
print("========================")
print(train_dataset[0])
print("========================")
print(train_dataset[0]['content'])

# Prepare the prompt using the 'content' of the first LeetCode question
question_prompt = train_dataset[0]['content']

# Append generation instruction
question_with_prompt = f"""# Request: Output should be in Python with explanation.
# Question:
{question_prompt}
# Write a Python function that solves this problem and explain each step with comments.
"""

# Load StarCoder2-3B
model_name = "bigcode/starcoder2-3b"

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Loading model...")
start = time.time()
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)
print(f"Model loaded in {time.time() - start:.2f} seconds.")

# Encode the prompt
inputs = tokenizer(question_with_prompt, return_tensors="pt").to(model.device)

# Generate code
start = time.time()
outputs = model.generate(
    inputs.input_ids,
    max_new_tokens=300,
    temperature=0.2,
    pad_token_id=tokenizer.eos_token_id
)
print(f"Generated in {time.time() - start:.2f} seconds.")

# Decode and print result
generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\n================ Generated Code ================\n")
print(generated_code)
print("\nStructural Analysis:")
analyze_generated_code(generated_code)

prompt = construct_prompt(train_dataset[0]['content'], problem_type="array")
print("ðŸ§¾ Prompt Preview:\n" + "="*40)
print(prompt)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
print(" Prompt length (in characters):", len(prompt))

def self_refine(prompt, model, tokenizer, max_new_tokens=300):
    """
    Step 1: Initial generation
    Step 2: Refine the generated code
    """
    # First generation
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output1 = model.generate(
        inputs.input_ids,
        max_new_tokens=max_new_tokens,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
    )
    code1 = tokenizer.decode(output1[0], skip_special_tokens=True)

    # Refinement
    refinement_prompt = f"""The following code was generated based on a programming problem:

{code1}

Please refine this code: fix bugs, improve readability, and add better comments if needed.
Provide only the improved version:"""

    inputs2 = tokenizer(refinement_prompt, return_tensors="pt").to(model.device)
    output2 = model.generate(
        inputs2.input_ids,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        pad_token_id=tokenizer.eos_token_id
    )
    code2 = tokenizer.decode(output2[0], skip_special_tokens=True)

    return code1, code2

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the model once
model_name = "bigcode/starcoder2-3b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# Prepare one LeetCode sample
problem_text = train_dataset[0]['content']

# Define multiple prompt strategies
prompt_variants = {
    "v1_basic": f"{problem_text}\n\n# Please solve this problem using Python and include comments.",
    "v2_docstring": f"### Task:\nWrite a Python function with detailed docstrings and inline comments.\n\n{problem_text}",
    "v3_plan_then_code": f"### Task:\nFirst explain the steps, then generate the code.\n\n{problem_text}"
}

# Generate and compare outputs
comment_stats = {}  # key: prompt label, value: number of comments
for label, prompt in prompt_variants.items():
    print(f"\n\n============================")
    print(f" Prompt Style: {label}")
    print(f"============================")

    original_code, refined_code = self_refine(prompt, model, tokenizer)

    print(" First Generation:\n", original_code)
    print("\nRefined Version:\n", refined_code)

    print("\n Structural Analysis:")
    num_comments = analyze_generated_code(refined_code)
    comment_stats[label] = num_comments

"""# Prompt Design Comparison â€“ Observations and Analysis
We evaluated three different prompt formats on the same LeetCode problem using the StarCoder2-3B model:

- **v1_basic**: A simple request to solve the problem with comments.
- **v2_docstring**: A request for a function with structured docstrings and inline explanations.
- **v3_plan_then_code**: A two-step instruction asking for a natural language explanation before generating code.

###  Key Observations:

1. **v1_basic** produced a correct solution with basic inline comments, but the clarity and structure of the explanation were minimal.

2. **v2_docstring** generated a function that included both meaningful docstrings and well-placed inline comments. The output was highly readable and aligned with educational goals.

3. **v3_plan_then_code** was the most verbose. It started with a high-level explanation in plain English and then provided the Python code. While the explanation helped understand the logic, the generated code was sometimes slightly less compact.

###  Conclusion:

- Prompt structure significantly influences the format and clarity of the modelâ€™s output.
- The **docstring-based prompt** (`v2_docstring`) achieved the best balance between explanation quality and code clarity.
- Multi-stage prompts like `v3_plan_then_code` can be useful for instructional settings, but may require additional post-processing.

This analysis supports our design goal: using well-structured prompts to encourage explainable, educational code generation.

"""

import matplotlib.pyplot as plt

# Extract labels and comment counts
labels = list(comment_stats.keys())
counts = list(comment_stats.values())

# Plot
plt.figure(figsize=(8, 5))
bars = plt.bar(labels, counts)
plt.xlabel("Prompt Style")
plt.ylabel("Number of Inline Comments")
plt.title("Comparison of Comment Quantity per Prompt Style")

# Annotate exact numbers above bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, int(yval), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""# Summary of Comment Quantity Analysis (Based on Automated Evaluation)
v2_docstring generated the highest number of inline comments, with a total of 21, indicating that explicitly instructing the model to include â€œdetailed docstrings and inline commentsâ€ has the strongest influence.

v1_basic also resulted in a moderate number of comments (16), showing that a simple instruction mentioning comments is reasonably effective.

v3_plan_then_code performed the worst, producing only 2 comments. This suggests that prompting the model to â€œfirst explain then codeâ€ may distract it from inserting inline annotations.

Conclusion:
Prompting with clear emphasis on commenting behaviorâ€”especially using terms like "inline comments" or "docstrings"â€”leads to more structurally annotated code. In contrast, vague or planning-oriented prompts do not effectively encourage the model to comment its code.
"""

model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")

# Self-refine all prompt variants
for label, prompt in prompt_variants.items():
    print(f"\n===== Prompt Style: {label} =====")

    # Run self-refinement
    first_output, refined_output = self_refine(prompt, model, tokenizer)

    # Show both versions
    print("\n--- First Generation ---\n")
    print(first_output)

    print("\n--- Refined Version ---\n")
    print(refined_output)

    # Optional: comment count analysis
    print("\n--- Structural Analysis ---")
    print("Comments in First Generation:", analyze_generated_code(first_output))
    print("Comments in Refined Version:", analyze_generated_code(refined_output))

"""## Final Analysis and Summary

In this project, we explored various prompt engineering strategies using StarCoder2 models to generate code solutions for LeetCode-style problems. We compared three prompt variants:

- **v1_basic**: A simple instruction requesting commented Python code.
- **v2_docstring**: A docstring-focused format encouraging inline documentation.
- **v3_plan_then_code**: A structured approach asking for a planning step before code generation.

We also implemented a **self-refinement** module, which takes the model's initial output and regenerates an improved version based on its own code and a fixed prompt template.

### Key Observations:

- The **plan-then-code** prompt (`v3`) generally produced more structured and readable code, especially when coupled with the self-refinement process.
- The **docstring prompt (`v2`)** resulted in code with the highest average comment density.
- The **self-refinement step** consistently improved the quality of comments, fixed formatting issues, and added missing logic steps in many cases.
- Visualizations confirmed that different prompts yield noticeably different levels of explanation and code quality.

### Conclusion:

Prompt design has a direct impact on both code correctness and explainability. Moreover, enabling models to revise their own output through self-refinement can meaningfully improve clarity and documentation. This demonstrates the potential of combining prompt engineering with feedback-driven refinement in building intelligent educational coding assistants.

"""

problem_text = train_dataset[0]['content']
final_code = self_refine_until_success(problem_text, model, tokenizer)
print("\nFinal Code:\n", final_code)

import pandas as pd


prompt_results = [
    {"Prompt": "v1_basic", "Comments": 16, "Docstring": "No", "Plan-then-Code": "No"},
    {"Prompt": "v2_docstring", "Comments": 21, "Docstring": "Yes", "Plan-then-Code": "No"},
    {"Prompt": "v3_plan_then_code", "Comments": 2, "Docstring": "No", "Plan-then-Code": "Yes"}
]

df = pd.DataFrame(prompt_results)
display(df)


df.to_csv("prompt_comparison.csv", index=False)



# =========================
# Metrics pipeline (English)
# =========================
#
# What it does:
# 1) Uses StarCoder to generate code for the Two Sum problem under 3 prompt styles.
# 2) Runs unit tests, then triggers a simple self-refinement step if tests fail.
# 3) Computes metrics for initial vs refined code:
#    - pass_rate (fraction of tests passed)
#    - comment_density (#comment lines / #non-empty lines)
#    - has_docstring (True/False)
#    - loc (non-empty lines of code)
# 4) Saves a CSV and draws two charts.

import re
import time
import json
import textwrap
from typing import List, Dict, Tuple

import torch
import pandas as pd
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForCausalLM

# ---------------------
# 0) Model preparation
# ---------------------
MODEL_NAME = "bigcode/starcoder2-3b"   # change if needed
DEVICE_MAP = "auto"
DTYPE = torch.float16 if torch.cuda.is_available() else torch.float32

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

print("Loading model...")
t0 = time.time()
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map=DEVICE_MAP,
    torch_dtype=DTYPE,
)
model.eval()
print(f"Model loaded in {time.time()-t0:.2f}s.")

# ----------------------------
# 1) Problem & unit test suite
# ----------------------------
TWO_SUM_DESC = textwrap.dedent("""
Given an array of integers nums and an integer target, return indices [i, j]
of the two numbers such that they add up to target. Each input has exactly one solution.
You may not use the same element twice. Return indices in any order.
""").strip()

# Test cases (order of indices does not matter)
TWO_SUM_TESTS = [
    {"input": ([2,7,11,15], 9), "expected_any": [{0,1}]},
    {"input": ([3,2,4], 6),     "expected_any": [{1,2}]},
    {"input": ([3,3], 6),       "expected_any": [{0,1}]},
    {"input": ([0,4,3,0], 0),   "expected_any": [{0,3}]},
    {"input": ([-1,-2,-3,-4,-5], -8), "expected_any": [{2,4}, {1,3}]},  # allow either valid pair
]

# -------------------------------------------------------
# 2) Prompt styles (all enforce â€œoutput ONLY one codeblockâ€
#    and the exact function signature to make parsing easy)
# -------------------------------------------------------
STRICT_SPEC = textwrap.dedent("""
Implement EXACTLY this function in Python:

def twoSum(nums: list[int], target: int) -> list[int]:
    \"\"\"
    Return indices [i, j] such that nums[i] + nums[j] == target.
    - No input() or prints
    - No class; top-level function only
    - Return a list of two indices
    - O(n) expected
    \"\"\"

Only output a single Python code block with the function.
""").strip()

def prompt_v1_basic(problem: str) -> str:
    return (
        f"### Problem\n{problem}\n\n"
        "Write the solution in Python with helpful inline comments.\n\n"
        f"{STRICT_SPEC}\n"
    )

def prompt_v2_docstring(problem: str) -> str:
    return (
        f"### Problem\n{problem}\n\n"
        "Write the solution in Python with a clear docstring and concise inline comments.\n\n"
        f"{STRICT_SPEC}\n"
    )

def prompt_v3_plan_then_code(problem: str) -> str:
    return (
        f"### Problem\n{problem}\n\n"
        "First reason briefly (internally). Then output ONLY the code block.\n"
        "Do not include your reasoning in the final output.\n\n"
        f"{STRICT_SPEC}\n"
    )

PROMPTS = {
    "v1_basic": prompt_v1_basic(TWO_SUM_DESC),
    "v2_docstring": prompt_v2_docstring(TWO_SUM_DESC),
    "v3_plan_then_code": prompt_v3_plan_then_code(TWO_SUM_DESC),
}

# ------------------------------
# 3) LLM generation helpers
# ------------------------------
def generate_code(prompt: str, max_new_tokens: int = 300, temperature: float = 0.2) -> str:
    """Call the model and return raw decoded text."""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(out[0], skip_special_tokens=True)

def extract_code_block(text: str) -> str:
    """
    Extract the first ```python ... ``` block. If none, try to grab the def twoSum(...) chunk.
    """
    fence = re.search(r"```python(.*?)```", text, flags=re.S|re.I)
    if fence:
        return fence.group(1).strip()
    # Fallback: find def twoSum(...) until end
    m = re.search(r"(def\s+twoSum\s*\(.*?\)\s*:[\s\S]+)", text)
    return m.group(1).strip() if m else text.strip()

# ------------------------------------
# 4) Static analysis for code comments
# ------------------------------------
def analyze_comments(code: str) -> Dict[str, float]:
    lines = [ln for ln in code.splitlines()]
    non_empty = [ln for ln in lines if ln.strip()]
    num_non_empty = max(1, len(non_empty))

    num_comment_lines = sum(1 for ln in non_empty if ln.strip().startswith("#"))
    has_docstring = bool(re.search(r'"""[\s\S]*?"""|\'\'\'[\s\S]*?\'\'\'', code))

    return {
        "loc": len(non_empty),
        "comment_lines": num_comment_lines,
        "comment_density": num_comment_lines / num_non_empty,
        "has_docstring": has_docstring,
    }

# ------------------------------
# 5) Execution & unit test runner
# ------------------------------
def run_two_sum(code: str, tests: List[Dict]) -> Tuple[float, str]:
    """
    Execute user's code in a sandbox namespace and run unit tests.
    Returns (pass_rate, error_report).
    """
    ns = {}
    try:
        exec(code, ns)
    except Exception as e:
        return 0.0, f"Compilation error: {type(e).__name__}: {e}"

    if "twoSum" not in ns or not callable(ns["twoSum"]):
        return 0.0, "Function twoSum not found."

    func = ns["twoSum"]
    passed = 0
    detail_errors = []

    for t in tests:
        args = t["input"]
        expected_any_sets = [set(pair) for pair in t["expected_any"]]
        try:
            out = func(*args)
        except Exception as e:
            detail_errors.append(f"Runtime error on input {args}: {type(e).__name__}: {e}")
            continue

        if not isinstance(out, list) or len(out) != 2:
            detail_errors.append(f"Invalid output type/shape on {args}: got {out}")
            continue

        if set(out) in expected_any_sets:
            passed += 1
        else:
            detail_errors.append(f"Wrong answer on {args}: got {out}, expected one of {t['expected_any']}")

    pass_rate = passed / len(tests)
    error_report = "OK" if passed == len(tests) else "\n".join(detail_errors[:5])
    return pass_rate, error_report

# ---------------------------------------
# 6) One-step self-refinement if tests fail
# ---------------------------------------
def refine_code(problem: str, previous_code: str, error_report: str, temperature: float = 0.2) -> str:
    refinement_prompt = textwrap.dedent(f"""
    You previously produced a solution for the problem below, but tests failed.

    ### Problem
    {problem}

    ### Your previous code
    ```python
    {previous_code}
    ```

    ### Test feedback (first errors)
    {error_report}

    Please provide a corrected implementation.
    {STRICT_SPEC}
    """).strip()

    raw = generate_code(refinement_prompt, temperature=temperature)
    return extract_code_block(raw)

# -----------------------------------------
# 7) Run all styles & collect the metrics
# -----------------------------------------
records = []

for label, prompt in PROMPTS.items():
    print(f"\n=== Evaluating: {label} ===")

    # initial generation
    raw_text = generate_code(prompt, temperature=0.2)
    gen_code = extract_code_block(raw_text)
    gen_static = analyze_comments(gen_code)
    gen_pass, gen_err = run_two_sum(gen_code, TWO_SUM_TESTS)

    # attempt one refinement if not perfect
    if gen_pass < 1.0:
        ref_code = refine_code(TWO_SUM_DESC, gen_code, gen_err, temperature=0.2)
        ref_static = analyze_comments(ref_code)
        ref_pass, ref_err = run_two_sum(ref_code, TWO_SUM_TESTS)
    else:
        ref_code = gen_code
        ref_static = gen_static
        ref_pass = gen_pass
        ref_err = "OK"

    print(f"Initial  â€” pass_rate: {gen_pass:.2f} | comment_density: {gen_static['comment_density']:.3f}")
    print(f"Refined  â€” pass_rate: {ref_pass:.2f} | comment_density: {ref_static['comment_density']:.3f}")

    records.append({
        "prompt": label,

        "gen_loc": gen_static["loc"],
        "gen_comment_lines": gen_static["comment_lines"],
        "gen_comment_density": round(gen_static["comment_density"], 6),
        "gen_has_docstring": gen_static["has_docstring"],
        "gen_pass_rate": round(gen_pass, 3),
        "gen_error_head": gen_err if gen_err == "OK" else gen_err.split("\n")[0],

        "ref_loc": ref_static["loc"],
        "ref_comment_lines": ref_static["comment_lines"],
        "ref_comment_density": round(ref_static["comment_density"], 6),
        "ref_has_docstring": ref_static["has_docstring"],
        "ref_pass_rate": round(ref_pass, 3),
        "ref_error_head": ref_err if ref_err == "OK" else ref_err.split("\n")[0],
    })

df = pd.DataFrame.from_records(records)
print("\n=== Metrics summary ===")
display(df)

# Save CSV for your paper/report
csv_path = "metrics_summary_two_sum.csv"
df.to_csv(csv_path, index=False)
print(f"Saved: {csv_path}")

# -----------------------------
# 8) Visualization (two charts)
# -----------------------------
plt.figure(figsize=(7, 4))
x = range(len(df))
plt.bar([i-0.2 for i in x], df["gen_pass_rate"], width=0.4, label="Initial")
plt.bar([i+0.2 for i in x], df["ref_pass_rate"], width=0.4, label="Refined")
plt.xticks(list(x), df["prompt"], rotation=15)
plt.ylabel("Pass rate")
plt.title("Initial vs Refined pass rate (Two Sum)")
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 4))
plt.bar([i-0.2 for i in x], df["gen_comment_density"], width=0.4, label="Initial")
plt.bar([i+0.2 for i in x], df["ref_comment_density"], width=0.4, label="Refined")
plt.xticks(list(x), df["prompt"], rotation=15)
plt.ylabel("Comment density (comments/line)")
plt.title("Initial vs Refined comment density")
plt.legend()
plt.tight_layout()
plt.show()