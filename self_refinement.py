# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tf4pQuBhwiOr9-9dvP_r_5U01zKN_ZQS
"""

def construct_prompt(problem_text, problem_type=None):
    base_prompt = f"""### Problem
{problem_text}

### Requirements
- Write clean, efficient Python code
- Include detailed inline comments
- Return a function only; no extra print statements

"""
    if problem_type == "array":
        base_prompt += "- Handle edge cases like empty arrays or duplicates\n"
    elif problem_type == "string":
        base_prompt += "- Optimize string operations and consider edge cases\n"
    elif problem_type == "graph":
        base_prompt += "- You may consider DFS or BFS traversal if needed\n"
    return base_prompt
# Example test
# print(construct_prompt("Given an array of nums...", problem_type="array"))

def analyze_generated_code(code):
    lines = code.strip().split("\n")
    num_comments = sum(1 for l in lines if l.strip().startswith("#"))
    has_docstring = '"""' in code or "'''" in code
    print(f"Total lines: {len(lines)}")
    print(f"Comments found: {num_comments}")
    print(f"Docstring present: {has_docstring}")
    return num_comments

# Install required packages
!pip install transformers datasets huggingface_hub

# Login to Hugging Face Hub (replace with your own token)
from huggingface_hub import login
login(token="hf_xxx...")  # replace with your own token  # Token removed for security  # ‚Üê replace with your real token

# Load necessary libraries
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch
import time

# Load the LeetCode dataset
ds = load_dataset("greengerong/leetcode")
train_dataset = ds['train']

# Print first sample (optional)
print("========================")
print(train_dataset[0])
print("========================")
print(train_dataset[0]['content'])

# Prepare the prompt using the 'content' of the first LeetCode question
question_prompt = train_dataset[0]['content']

# Append generation instruction
question_with_prompt = f"""# Request: Output should be in Python with explanation.
# Question:
{question_prompt}
# Write a Python function that solves this problem and explain each step with comments.
"""

# Load StarCoder2-3B
model_name = "bigcode/starcoder2-3b"

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Loading model...")
start = time.time()
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.float16)
print(f"Model loaded in {time.time() - start:.2f} seconds.")

# Encode the prompt
inputs = tokenizer(question_with_prompt, return_tensors="pt").to(model.device)

# Generate code
start = time.time()
outputs = model.generate(
    inputs.input_ids,
    max_new_tokens=300,
    temperature=0.2,
    pad_token_id=tokenizer.eos_token_id
)
print(f"Generated in {time.time() - start:.2f} seconds.")

# Decode and print result
generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\n================ Generated Code ================\n")
print(generated_code)
print("\nStructural Analysis:")
analyze_generated_code(generated_code)

prompt = construct_prompt(train_dataset[0]['content'], problem_type="array")
print("üßæ Prompt Preview:\n" + "="*40)
print(prompt)

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
print(" Prompt length (in characters):", len(prompt))

def self_refine(prompt, model, tokenizer, max_new_tokens=300):
    """
    Step 1: Initial generation
    Step 2: Refine the generated code
    """
    # First generation
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output1 = model.generate(
        inputs.input_ids,
        max_new_tokens=max_new_tokens,
        temperature=0.3,
        pad_token_id=tokenizer.eos_token_id
    )
    code1 = tokenizer.decode(output1[0], skip_special_tokens=True)

    # Refinement
    refinement_prompt = f"""The following code was generated based on a programming problem:

{code1}

Please refine this code: fix bugs, improve readability, and add better comments if needed.
Provide only the improved version:"""

    inputs2 = tokenizer(refinement_prompt, return_tensors="pt").to(model.device)
    output2 = model.generate(
        inputs2.input_ids,
        max_new_tokens=max_new_tokens,
        temperature=0.2,
        pad_token_id=tokenizer.eos_token_id
    )
    code2 = tokenizer.decode(output2[0], skip_special_tokens=True)

    return code1, code2

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the model once
model_name = "bigcode/starcoder2-3b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")

# Prepare one LeetCode sample
problem_text = train_dataset[0]['content']

# Define multiple prompt strategies
prompt_variants = {
    "v1_basic": f"{problem_text}\n\n# Please solve this problem using Python and include comments.",
    "v2_docstring": f"### Task:\nWrite a Python function with detailed docstrings and inline comments.\n\n{problem_text}",
    "v3_plan_then_code": f"### Task:\nFirst explain the steps, then generate the code.\n\n{problem_text}"
}

# Generate and compare outputs
comment_stats = {}  # key: prompt label, value: number of comments
for label, prompt in prompt_variants.items():
    print(f"\n\n============================")
    print(f" Prompt Style: {label}")
    print(f"============================")

    original_code, refined_code = self_refine(prompt, model, tokenizer)

    print(" First Generation:\n", original_code)
    print("\nRefined Version:\n", refined_code)

    print("\n Structural Analysis:")
    num_comments = analyze_generated_code(refined_code)
    comment_stats[label] = num_comments

"""# Prompt Design Comparison ‚Äì Observations and Analysis
We evaluated three different prompt formats on the same LeetCode problem using the StarCoder2-3B model:

- **v1_basic**: A simple request to solve the problem with comments.
- **v2_docstring**: A request for a function with structured docstrings and inline explanations.
- **v3_plan_then_code**: A two-step instruction asking for a natural language explanation before generating code.

###  Key Observations:

1. **v1_basic** produced a correct solution with basic inline comments, but the clarity and structure of the explanation were minimal.

2. **v2_docstring** generated a function that included both meaningful docstrings and well-placed inline comments. The output was highly readable and aligned with educational goals.

3. **v3_plan_then_code** was the most verbose. It started with a high-level explanation in plain English and then provided the Python code. While the explanation helped understand the logic, the generated code was sometimes slightly less compact.

###  Conclusion:

- Prompt structure significantly influences the format and clarity of the model‚Äôs output.
- The **docstring-based prompt** (`v2_docstring`) achieved the best balance between explanation quality and code clarity.
- Multi-stage prompts like `v3_plan_then_code` can be useful for instructional settings, but may require additional post-processing.

This analysis supports our design goal: using well-structured prompts to encourage explainable, educational code generation.

"""

import matplotlib.pyplot as plt

# Extract labels and comment counts
labels = list(comment_stats.keys())
counts = list(comment_stats.values())

# Plot
plt.figure(figsize=(8, 5))
bars = plt.bar(labels, counts)
plt.xlabel("Prompt Style")
plt.ylabel("Number of Inline Comments")
plt.title("Comparison of Comment Quantity per Prompt Style")

# Annotate exact numbers above bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.2, int(yval), ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""# Summary of Comment Quantity Analysis (Based on Automated Evaluation)
v2_docstring generated the highest number of inline comments, with a total of 21, indicating that explicitly instructing the model to include ‚Äúdetailed docstrings and inline comments‚Äù has the strongest influence.

v1_basic also resulted in a moderate number of comments (16), showing that a simple instruction mentioning comments is reasonably effective.

v3_plan_then_code performed the worst, producing only 2 comments. This suggests that prompting the model to ‚Äúfirst explain then code‚Äù may distract it from inserting inline annotations.

Conclusion:
Prompting with clear emphasis on commenting behavior‚Äîespecially using terms like "inline comments" or "docstrings"‚Äîleads to more structurally annotated code. In contrast, vague or planning-oriented prompts do not effectively encourage the model to comment its code.
"""

model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")

# Self-refine all prompt variants
for label, prompt in prompt_variants.items():
    print(f"\n===== Prompt Style: {label} =====")

    # Run self-refinement
    first_output, refined_output = self_refine(prompt, model, tokenizer)

    # Show both versions
    print("\n--- First Generation ---\n")
    print(first_output)

    print("\n--- Refined Version ---\n")
    print(refined_output)

    # Optional: comment count analysis
    print("\n--- Structural Analysis ---")
    print("Comments in First Generation:", analyze_generated_code(first_output))
    print("Comments in Refined Version:", analyze_generated_code(refined_output))

"""## Final Analysis and Summary

In this project, we explored various prompt engineering strategies using StarCoder2 models to generate code solutions for LeetCode-style problems. We compared three prompt variants:

- **v1_basic**: A simple instruction requesting commented Python code.
- **v2_docstring**: A docstring-focused format encouraging inline documentation.
- **v3_plan_then_code**: A structured approach asking for a planning step before code generation.

We also implemented a **self-refinement** module, which takes the model's initial output and regenerates an improved version based on its own code and a fixed prompt template.

### Key Observations:

- The **plan-then-code** prompt (`v3`) generally produced more structured and readable code, especially when coupled with the self-refinement process.
- The **docstring prompt (`v2`)** resulted in code with the highest average comment density.
- The **self-refinement step** consistently improved the quality of comments, fixed formatting issues, and added missing logic steps in many cases.
- Visualizations confirmed that different prompts yield noticeably different levels of explanation and code quality.

### Conclusion:

Prompt design has a direct impact on both code correctness and explainability. Moreover, enabling models to revise their own output through self-refinement can meaningfully improve clarity and documentation. This demonstrates the potential of combining prompt engineering with feedback-driven refinement in building intelligent educational coding assistants.

"""

